---
title: Celery
date: 2023-08-21 10:55:23
permalink: /pages/2ea63b/
categories:
  - languages
  - python
tags:
  - 
---

## Celery基本使用
### 安装
```sh
pip3 install celery
# windows下安装
pip3 install eventlet
```
### 启动
```sh
celery -A celery_task worker -l info
# windows
celery -A celery_task worker -l info -P eventlet
```
| 参数 | 说明 |
|:-:|:-:|
|-A/--app|要使用的应用程序实例|
|-n/--hostname|设置自定义主机名|
|-Q/--queues|指定一个消息队列，该进程只接受此队列的任务|
|-l/--loglevel|定义打印log的等级 DEBUG, INFO, WARNING, ERROR, CRITICAL, FATAL|
|-c/--concurrency|同时处理任务的工作进程数量，默认值是系统上可用的cpu数量|
|-B/--beat|定义运行celery打周期任务调度程序|
|–autoscale|池的进程的最大数量和最小数量|
|–max-tasks-per-child|配置工作单元子进程在被一个新进程取代之前可以执行的最大任务数量|
|–max-memory-per-child|设置工作单元子进程被替换之前可以使用的最大内存|
### celery
初始化celery对象。
```python
from celery import Celery

# rabbitMQ
broker_url = 'amqp://myuser:mypassword@localhost:5672/myvhost'
# redis
broker_url = 'redis://:password@hostname:port/db_number'
backend_url = ''
app = Celery(broker=broker_url, backend=backend_url, include=['celery_task.tasks'])
```
定时任务配置
```python 
# 时区  
app.conf.timezone = 'Asia/Shanghai'  
# 是否使用UTC  
app.conf.enable_utc = False # 这个为False，则走django项目的settings文件的国际化配置 

# 任务的定时配置  
from celery.schedules import crontab  
app.conf.beat_schedule = {  
	'low-task': {  
		'task': 'celery_task.tasks.test',  
		'schedule': crontab(hour=8, day_of_week=1), # 每周一早八点  
		'args': (300, 150),  
	}  
}
```
### tasks
构建任务
```python
from .celery import app  
  
@app.task  
def test(n, m):  
	print('n+m的结果：%s' % (n + m))  
	return n + m
```
内置钩子函数
```python
class demotask(Task):
    # 任务成功执行
    def on_success(self, retval, task_id, *args, **kwargs):
        logger.info(f'task id:{task_id}, arg:{args}, successful!')

    # 任务失败执行
    def on_failure(self, exc, task_id, *args, **kwargs, einfo):
        logger.info(f'task id:{task_id}, arg:{args}, failed! erros:{exc}')

    # 任务重试执行
    def on_retry(self, exc, task_id, *args, **kwargs, einfo):
        logger.info(f'task id:{task_id}, arg:{args}, retry! einfo:{exc}')

@app.task(base=demotask, bind=True)  
def test(n, m):  
	print('n+m的结果：%s' % (n + m))  
	return n + m
```
### 添加任务
```python
from celery_task import tasks

# 添加立即执行任务 
task = tasks.test.delay(10, 20)
# task.id可获取任务id

# 添加延迟任务  
from datetime import datetime, timedelta  
eta = datetime.utcnow() + timedelta(seconds=10)  
tasks.test.apply_async(args=(200, 50), eta=eta)
```
### 获取任务状态/结果
```python
from celery_task.celery import app  
from celery.result import AsyncResult


id = 'bfa2cff3-cea1-4938-b231-75bb05bf9a73'  
result = AsyncResult(id=id, app=app)  
if result.successful():  
	result = result.get()  
	print(result)  
elif result.failed():  
	print('任务失败')
```
### Supervisor进程管理
```ini
[program: {{.app_code}}_celery_home_applications]  
command = /cache/.bk/env/bin/python {{.app_container_path}}code/manage.py celery worker -Q push -n {{.node_name}}_{{.app_code}}_comsumer -l INFO -c 4 --maxtasksperchild=128  
directory = {{.app_container_path}}code/  
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log  
redirect_stderr = true  
stopwaitsecs = 10  
autorestart = true  
environment = {{.environment}}  
  
# 协程 并发方式启动 执行io密集型任务的 worker
[program: {{.app_code}}_celery_pipeline1]  
command = /cache/.bk/env/bin/python {{.app_container_path}}code/manage.py celery worker -Q insert_data -n {{.node_name}}_{{.app_code}}_pipeline1 -l INFO -P gevent -c 128 --maxtasksperchild=128  
directory = {{.app_container_path}}code/  
stdout_logfile = {{.app_container_path}}logs/{{.app_code}}/celery.log  
redirect_stderr = true  
stopwaitsecs = 10  
autorestart = true  
environment = {{.environment}}
```
```sh
# 启动Supervisor
supervistord -c /etc/supervisord.conf
# 进入supervistor管理
supervistorctl
# 启动名为name的program
start name
# 全部启动
start all
# 全部停止
stop all
# 全部重启
restart all
```