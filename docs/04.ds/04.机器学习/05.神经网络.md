---
title: 神经网络
date: 2023-05-23 16:02:31
permalink: /pages/70494e/
categories:
  - ds
  - 机器学习
tags:
  - 
---


## 激活函数
**激活函数（activation function）**通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。
### ReLU函数
修正线性单元（Rectified linear unit，_ReLU_）ReLU函数通过将相应的活性值设为0，仅保留正元素并丢弃所有负元素
```python
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)
```
实现relu函数
```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)
```

### sigmoid函数
对于一个定义域在R中的输入， _sigmoid函数_将输入变换为区间(0, 1)上的输出。 因此，sigmoid通常称为_挤压函数_（squashing function）： 它将范围（-inf, inf）中的任意输入压缩到区间（0, 1）中的某个值
```python
y = torch.sigmoid(x)
```

### tanh函数
与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上
```python
y = torch.tanh(x)
```