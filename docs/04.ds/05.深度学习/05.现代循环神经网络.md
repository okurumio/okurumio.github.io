---
title: 现代循环神经网络
date: 2023-06-25 14:44:35
permalink: /pages/3b193b/
categories:
  - ds
  - 深度学习
tags:
  - 
---


## LSTM
长短期记忆网络
```python
torch.nn.LSTM(_*args_, _**kwargs_)
```
### 参数
- input_size: 输入的特征维度
- output_size: 输出的特征维度
- num_layers: 层数（注意与时序展开区分）
- bidirectional: 如果为`True`，为双向LSTM。默认为`False`

LSTM的输入：input,(h0,c0)
- input(seq_len,batch,input_size): 包含输入特征的`tensor`,注意输入是`tensor`。
- h0: 保存初始化隐藏层状态的`tensor`
- c0: 保存初始化细胞状态的`tensor`

LSTM的输出： output,(hn,cn)
- output(seq_len, batch, hidden_size * num_directions): 保存`RNN`最后一层输出的`tensor`
- hn: 保存`RNN`最后一个时间步隐藏状态的`tensor`
- cn: 保存`RNN`最后一个时间步细胞状态的`tensor`
### LSTM模型
```python
class LSTMtagger(nn.Module):  
    def __init__(self,embedding_dim,hidden_dim,vocab_size,tagset_size):  
        super(LSTMtagger,self).__init__()  
        self.hidden_dim = hidden_dim 
        #随机初始化词向量表，是神经网络的参数
        self.word_embeddings = nn.Embedding(vocab_size,embedding_dim)
        #实例化一个LSTM单元，单元输入维度是embedding_dim，输出维度是hidden_dim  
        self.lstm = nn.LSTM(embedding_dim,hidden_dim)
        #线性层从隐藏状态空间映射到标签空间
        self.hidden2tag = torch.Linear(hidden_dim,tagset_size) 
        
    def forward(self,sentence):  
	    #查询句子的词向量表示。输入应该是二维tensor。  
        embeds = self.word_embeddings(sentence) 
        lstm_out,hidden = self.lstm(embeds.view(len(sentence),1,-1))  
        tag_space = self.hidden2tag(lstm_out.view(len(sentence),-1))  
        tag_scores = F.log_softmax(tag_space)
```
## seq2seq
分为encoder层与decoder层，在encode阶段，第一个节点输入一个词，之后的节点输入的是下一个词与前一个节点的hidden state，最终encoder会输出一个context，这个context又作为decoder的输入，每经过一个decoder的节点就输出一个翻译后的词，并把decoder的hidden state作为下一层的输入。
```python
class Seq2Seq(nn.Module):
	def __init__(self):
	super(Seq2Seq, self).__init__()
	self.encoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)
	self.decoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)
	self.fc = nn.Linear(n_hidden, n_class)

	def forward(self, enc_input, enc_hidden, dec_input):
		enc_input = enc_input.transpose(0, 1)
		dec_input = dec_input.transpose(0, 1)
		 _, h_t = self.encoder(enc_input, enc_hidden)
		outputs, _ = self.decoder(dec_input, h_t)
		model = self.fc(outputs)
		return model
```

## Attention
注意力机制
1. encoder提供了更多的数据给到decoder，encoder会把所有的节点的hidden state提供给decoder，而不仅仅只是encoder最后一个节点的hidden state。
2. decoder并不是直接把所有encoder提供的hidden state作为输入，而是采取一种选择机制，把最符合当前位置的hidden state选出来，具体的步骤如下：
	- 确定哪一个hidden state与当前节点关系最为密切
	- 计算每一个hidden state的分数值（具体怎么计算我们下文讲解）
	- 对每个分数值做一个softmax的计算，这能让相关性高的hidden state的分数值更大，相关性低的hidden state的分数值更低
## Transformers
### nn.Transformer
```python
torch.nn.Transformer(
	_d_model=512_, 
	_nhead=8_, 
	_num_encoder_layers=6_, 
	_num_decoder_layers=6_, 
	_dim_feedforward=2048_, 
	_dropout=0.1_, 
	_activation=<function relu>_, 
	_custom_encoder=None_, 
	_custom_decoder=None_, 
	_layer_norm_eps=1e-05_, 
	_batch_first=False_,
	_norm_first=False_, 
	_device=None_, 
	_dtype=None_
)
```
参数：
- d_model: Encoder和Decoder输入参数的特征维度，也就是词向量的维度，默认为512。
- nhead: 多头注意力机制中，head的数量，默认值为8。
- num_encoder_layers: TransformerEncoderLayer的数量。该值越大，网络越深，网络参数量越多，计算量越大，默认值为6。
- num_decoder_layers：TransformerDecoderLayer的数量。该值越大，网络越深，网络参数量越多，计算量越大，默认值为6。
- dim_feedforward：Feed Forward层（Attention后面的全连接网络）的隐藏层的神经元数量。该值越大，网络参数量越多，计算量越大。默认值为2048。
- dropout：dropout值。默认值为0.1。
- activation： Feed Forward层的激活函数。取值可以是string(“relu” or “gelu”)或者一个一元可调用的函数。默认值是relu。
- custom_encoder：自定义Encoder，默认值为None。
- custom_decoder: 自定义Decoder，默认值为None。
- layer_norm_eps: Add&Norm层中，BatchNorm的eps参数值。默认为1e-5。
- batch_first：batch维度是否是第一个。如果为True，则输入的shape应为(batch_size, 词数，词向量维度)，否则应为(词数, batch_size, 词向量维度)。默认为False。
- norm_first – 是否要先执行norm。
### forward
- src: Encoder的输入。也就是将token进行Embedding并Positional Encoding之后的tensor。必填参数。Shape为(batch_size, 词数, 词向量维度)
- tgt: 与src同理，Decoder的输入。 必填参数。Shape为(词数, 词向量维度)
- src_mask: 对src进行mask。不常用。Shape为(词数, 词数)
- tgt_mask：对tgt进行mask。常用。Shape为(词数, 词数)
- memory_mask – 对Encoder的输出memory进行mask。 不常用。Shape为(batch_size, 词数, 词数)
- src_key_padding_mask：对src的token进行mask. 常用。Shape为(batch_size, 词数)
- tgt_key_padding_mask：对tgt的token进行mask。常用。Shape为(batch_size, 词数)
- memory_key_padding_mask：对tgt的token进行mask。不常用。Shape为(batch_size, 词数)
### Transformers实现
- Embedding: 负责将token映射成高维向量。
- Positional Encoding：位置编码。用于为token编码增加位置信息。
- Linear+Softmax：一个线性层加一个Softmax，用于对nn.Transformer输出的结果进行token预测。
```python
# 定义src和tgt
src = torch.LongTensor([[0, 3, 4, 5, 6, 7, 8, 1, 2, 2]])
tgt = torch.LongTensor([[0, 3, 4, 5, 6, 7, 8, 1, 2]])

# 生成src_key_padding_mask和tgt_key_padding_mask
def get_key_padding_mask(tokens):
    key_padding_mask = torch.zeros(tokens.size())
    key_padding_mask[tokens == 2] = -torch.inf
    return key_padding_mask

src_key_padding_mask = get_key_padding_mask(src)
tgt_key_padding_mask = get_key_padding_mask(tgt)
# 生成tgt_mask
tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(-1)) 
# 定义编码器
embedding = nn.Embedding(10, 128)
transformer = nn.Transformer(d_model=128, batch_first=True)
outputs = transformer(embedding(src), embedding(tgt),
                      tgt_mask=tgt_mask,
                      src_key_padding_mask=src_key_padding_mask,
                      tgt_key_padding_mask=tgt_key_padding_mask)
```
## bert
### tokenizer
```python
tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-chinese')
```
tokenizer.encode()
```python
tokenizer.encode(
	text: str,
	text_pair: Optional[str] = None,   
	add_special_tokens: bool = True, 
	max_length: Optional[int] = None,  
	stride: int = 0,
	truncation_strategy: str = "longest_first",
	pad_to_max_length: bool = False,
	return_tensors: Optional[str] = None,
	**kwargs
)
```
### model
```python
class Bert(nn.Module):
    def __init__(self, dropout=0.5):
        super(Bert, self).__init__()
        self.bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-chinese')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 5)
        self.relu = nn.ReLU()

    def forward(self, input_id, mask):
        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        final_layer = self.relu(linear_output)
        return final_layer
```