---
title: 提高爬虫速度
date: 2022-09-06 14:48:13
permalink: /ds/spider/rocketspider/
categories: 
  - spider
tags: 
  - spider
  - python
---

**测试一下怎么能让爬虫速度最快**  
测试异步多协程爬虫，多线程爬虫，多进程爬虫，多进程异步爬虫在抓取批量url和分页场景下的爬虫性能。因为url请求耗时不定所以使用sleep代替请求。
## 异步爬虫
**网络请求库使用httpx. 限制协程数量3个. 不翻页. 不使用消息队列将url放入list中，或使用队列直接读出队列中所有任务.**
```python
import asyncio
import httpx
import time

class AsyncSpider:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36',
        }
        self.queue = redisdb()
        self.sem = asyncio.Semaphore(3)
        self.max_tries = 3
        self.redis_key = 'queue'
        
    async def crawl(self, url):
        async with self.sem:
            async with httpx.AsyncClient(timeout=3, verify=False) as client:
                tries = 0
                while tries < self.max_tries:
                    # await asyncio.sleep(0.1)
                    try:
                        response = await client.get(url, headers=self.headers)
                        if response.status_code == 200:
                            return response.response
                        else:
                            tries += 1
                            print('{}:{}'.format(url, response.status_code))
                    except:
                        tries += 1
                        print('{}:err'.format(url))

    async def work(self):
        tasks = []
        crawl_list = self.queue.zrange(self.redis_key, 0, -1, withscores=True)
        for url in crawl_list:
            tasks.append(asyncio.create_task(self.crawl(url)))
        result = await asyncio.gather(*tasks)
        print(result)
    
    def run(self):
        asyncio.run(self.work())
        
if __name__ == '__main__':
    start = time.perf_counter()
    spider = AsyncSpider()
    spider.run()
    print(f'Cost: {time.perf_counter() - start}')
```
250个url每个耗时1s共84.88032360002398s,最快的方法

**开启三个协程抓取. 不翻页. 使用消息队列循环获取url.**
```python
class AsyncSpider:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36',
        }
        self.queue = redisdb()
        self.max_tries = 3
        self.redis_key = 'queue'
    
    async def crawl(self, url):
        async with httpx.AsyncClient() as client:
            tries = 0
            while tries < self.max_tries:
                # await asyncio.sleep(0.1)
                try:
                    response = await client.get(url, headers=self.headers)
                    if response.status_code == 200:
                        return response.status_code
                    else:
                        tries += 1
                        print('{}:{}'.format(url, response.status_code))
                except:
                    tries += 1
                    print('{}:err'.format(url))

    async def consumer(self):
        while 1:
            url = self.queue.rpop(self.redis_key)
            # url = self.urls_queue.brpop(self.redis_key, 0)  # 监听队列，无数据时阻塞
            if url:
                result = await self.crawl(url.decode('utf8'))
                print(result)
            else:
                break

    async def work(self):
        tasks = []
        for i in range(3):
            tasks.append(asyncio.create_task(self.consumer()))
        res = await asyncio.gather(*tasks)

    def run(self):
        asyncio.run(self.work())
```
250个url每个耗时1s共88.28432169998996s

**限制协程数量3个.每个url翻10页. 使用消息队列循环获取url.之后构造所有页的url放入任务.**
```python
class AsyncSpider:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36',
        }
        self.queue = redisdb()
        self.sem = asyncio.Semaphore(3)
        self.max_pages = 30
        self.max_tries = 3
        self.redis_key = 'queue'

    async def crawl(self, client, url):  # 进行请求，并执行 retry
        async with self.sem:
            tries = 0
            while tries < self.max_tries:
                # await asyncio.sleep(0.1)
                try:
                    response = await client.get(url, headers=self.headers)
                    if response.status_code == 200:
                        return response.status_code
                    else:
                        tries += 1
                        print('{}:{}'.format(url, response.status_code))
                except:
                    tries += 1
                    print('{}:err'.format(url))

    async def work(self):
        while True:
            task = await self.queue.lpop(self.redis_key)
            if task:
                async with httpx.AsyncClient() as client:
                    tasks = []
                    for page in range(self.max_pages):
                        url = task.decode('utf8')+str(page)
                        tasks.append(asyncio.create_task(self.crawl(client, url)))
                    await asyncio.gather(*tasks)
            else:
                break

    def run(self):
        asyncio.run(self.work())
```
25个url每个耗时1s翻10页共102.04082680004649，因为在最后一页抓取完前其他协程会阻塞所以慢一些。

**开启三个协程抓取. 每个url翻10页. 使用消息队列循环获取url. 之后每个协助内循环翻页抓取.**
```python
class AsyncSpider:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36',
        }
        self.queue = redisdb()
        self.sem = asyncio.Semaphore(3)
        self.max_pages = 30
        self.max_tries = 3
        self.redis_key = 'queue'

    async def crawl(self, url):
        async with httpx.AsyncClient() as client:
            tries = 0
            while tries < self.max_tries:
                await asyncio.sleep(0.1)
                try:
                    response = await client.get(url, headers=self.headers)
                    if response.status_code == 200:
                        return response.status_code
                    else:
                        tries += 1
                        print('{}:{}'.format(url, response.status_code))
                except:
                    tries += 1
                    print('{}:err'.format(url))

    async def consumer(self):
        while 1:
            task = self.queue.rpop(self.redis_key)
            # url = self.urls_queue.brpop(self.redis_key, 0)  # 监听队列，无数据时阻塞
            if task:
                print(task.decode('utf8'))
                for page in range(self.max_pages):
                    url = task.decode('utf8')+str(page)
                    result = await self.crawl(url)
                    print(result)
            else:
                break

    async def work(self):
        tasks = []
        for i in range(3):
            tasks.append(asyncio.create_task(self.consumer()))
        res = await asyncio.gather(*tasks)

    def run(self):
        asyncio.run(self.work())
```
25个url每个耗时1s翻10页共96.03977949998807s

将url放入内存中会更快（废话），在需要翻页时限制协程数量会因为最后一页未抓取完阻塞其他协程，自己控制协程数会在处理最后的任务时导致其他协程空转。
将所有url构造好后放入内存统一抓取速度最快
## 多线程爬虫
**网络请求库使用requests. 限制线程数量3个.抓取25个url,每个翻10页**
```python
import threading
import time
sem = threading.Semaphore(3)

class ThreadSpider(threading.Thread):
    def __init__(self, i):
        threading.Thread.__init__(self)
        self.threadId = i
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36',
        }
        self.queue = redisdb()
        self.max_pages = 10
        self.max_tries = 3
        self.redis_key = 'queue'

    def crawl(self, url):
        for page in range(self.max_pages):
            print(url)
            time.sleep(1)

    def run(self):
        with sem:
            task = self.queue.lpop(self.redis_key)
            url = task.decode('utf-8')
            self.crawl(url)


def main():
    spiders = []
    for i in range(25):
        spiders.append(ThreadSpider(i))
    for spider in spiders:
        spider.start()
    for spider in spiders:
        spider.join()
```
25个url每个耗时1s翻10页共91.01745559996925s

**启动3个线程.抓取250个url**
```python
import threading
import time

class multiSpider(threading.Thread):
    def __init__(self, i):
        threading.Thread.__init__(self)
        self.threadId = i
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.0.0 Safari/537.36',
        }
        self.queue = redisdb()
        self.max_pages = 10
        self.max_tries = 3
        self.redis_key = 'queue'

    def crawl(self, url):
        print(url)
        time.sleep(1)

    def run(self):
        while 1:
            task = self.queue.lpop(self.redis_key)
            if task:
                url = task.decode('utf-8')
                self.crawl(url)
            else:
                break

def main():
    spiders = []
    for i in range(3):
        spiders.append(ThreadSpider(i))
    for spider in spiders:
        spider.start()
    for spider in spiders:
        spider.join()
```
250个url每个耗时1s共85.48093790002167s

## 多进程爬虫
