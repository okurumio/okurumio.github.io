---
title: scrapy
date: 2022-08-18 22:16:13
permalink: /ds/spider/scrapy/
categories: 
  - spider
tags: 
  - python
  - spider
  - 框架
---

## 概念
### 架构
**Engine：** 用来处理整个系统的数据流和事件，是整个框架的核心，可以理解为整个框架的中央处理器，负责数据的流转和逻辑的处理。  
**Item：** 爬取结果的数据结构，爬取的数据会被赋值成item对象。每个item就是一个类，类里面定义了爬取结果的数据字段，可以理解为用来规定爬取数据的存储格式。   
**Scheduler：** 用来接受Engine发过来的Request将其加人队列中，同时也可以将Request发回给Engine供Downloader执行，它主要维扩Request的调度逻辑，比如先进先出、先进后出、优先级进出等等。  
**Spiders：** 每个Spider里面定义了站点的爬取逻辑和页面的解析规则，它主要负责解析响应并生成 Item和新的请求然后发给 Engine 进行处理。  
**Downloader：** 完成“向服务器发送请求，然后拿到响应”的过程，得到的响应会再发送给 Engine 处理。  
**Item Pipelines：** Iterm Pipeline主要负责处理由Spider从页面中抽取的Item，做一此数据清洗、验证和存储等工作，比如将Item的某此字段进行规整，将Item存储到数据库等操作都可以由Item Pipeline来完成。  
**Downloader Middlewares：** 包含多个Downloader Miadleware它是位于Engine和Downloader之间的Hook框架，负责实现Downloader和Engine之间的请求和响应的处理过程。  
**Spide Miadlewares：** 位于Engine和Spiders之间的Hook框架，负责实现Spiders和Engine之间的item请求和响应的处理过程。
### 数据流
1. 启动爬虫项目时，Engine根据要爬取的目标站点找到处理该站点的Spider,Spider会生成最初需要爬取的页面对应的一个或多个Request，然后发给Engine。
2. Engine从Spider中获取这些Request，然后把它们交给Scheduler等待被调度。
3. Engine向Scheduler索取下一个要处理的Request，这时候scheduler根据其调度逻辑选择合适的Request发送给Engine。
4. Engine将Scheduler发来的Request转发给Downloader进行下载执行，将Request发送给Downloader的过程会经由许多定义好的Downloader Middlewares的处理。
5. Downloader将Request发送给目标服务器，得到对应的Response然后将其返回给Engine。将Response返回Engine的过程同祥会经由许多定义好的Downloader Middlewares的处理。
6. Engine以Downloder处接收到的Response里包含了爬取的目标站点的内容，Engine会将此Response发送给对应的Spider进行处理，将Response发送给Spider的过程中会经由定义好的Spider Middlewares的处理。 
7. Spider处理Response，解析Response的内容，这时候Spider会产生一个或多个爬取结果Item或者后续要爬取的目标页面对应的一个或多个Request,然后再将这些Item或Request发送给Engine进行处理，将Item或Request发送给Engine的过程会经由定义好的Spider Middlewares的处理。
8. Engine将Spider发回的一个或多个Item 转发给定义好的Item Pipelines进行数据处理或存储的一系列操作，将Spider发回的一个或多个Request转发Scheduler等待下一次被调度。
## 使用
```shell
# 创建一个scrapy项目
scrapy startproject mySpider
# 生成一个爬虫
scrapy genspider demo "demo.com"
# 运行爬虫
scrapy crawl demo 
```
运行爬虫脚本
```python
from scrapy import cmdline
cmdline.execute("scrapy crawl qb".split())
```
### settings
**引入settings**
```python
from scrapy.utils.project import get_project_settings
setting = get_project_settings()
```
**日志**
```python
# CRITICAL - 严重错误 ERROR - 一般错误 WARNING - 警告信息 INFO - 一般信息 DEBUG - 调试信息
LOG_LEVEL = 'ERROR'
LOG_FILE = 'log.txt'
```
**重试**
```python
DOWNLOADER_MIDDLEWARES = {
   'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,
}
# Retry settings
RETRY_ENABLED = True
# 重试次数
RETRY_TIMES = 3
# 重试HTTPCODES
RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 400, 404, 403]
```
**请求**
```python
# 全局最大并发数(default: 16)
CONCURRENT_REQUESTS = 32
# 单个域名最大并发数(default: 8)
CONCURRENT_REQUESTS_PER_DOMAIN = 16
# 单个ip最大并发数(default: 0)
CONCURRENT_REQUESTS_PER_IP = 16
# 下载延时，高并发采集时设为0(default: 0)
#DOWNLOAD_DELAY = 3
# 超时
DOWNLOAD_TIMEOUT = 3
```
### items
```python
class TestItem(scrapy.Item):
    name = scrapy.Field()
    age = scrapy.Field(serializer=int)
```
**dataclass**
```python
from dataclasses import dataclass

@dataclass
class TestItem:
    name_field: str
    age_field: int
```
**ItemLoader**
```python
from scrapy.loader import ItemLoader
from xxxx.items import TestItem

l = ItemLoader(item=TestItem(), response=response)
l.item  # 加载器解析的Item对象。
l.context  # ItemLoader的数据详细内容。
l.default_item_class  # 实例化方法。
l.default_input_processor  # 指定字段的默认输入处理器。
l.default_output_processor  # 指定字段的默认输出处理器。
l.default_selector_class  # 用于构造 selector 中的 ItemLoader。
l.selecto  # 在 Selector 对象中提取数据。
l.add_css (field_name, CSS, *processors, kw)  # 用于提取数据，通过 processors 和 kwargs 的处理后，存储到 field_name。
l.add_value(field_name, value, *processors, kw)  # 处理添加给定的 value 对于给定的字段。
l.add_xpath(field_name, value, *processors, kw)  # 用于与此相关的选择器中提取字符串列表。
l.get_collected_values(field_name)  # 返回字段的收集值。
l.get_css (CSS, *processors, kw)  # 用于从与此相关的选择器中提取 Unicode 字符串列表。
l.get_output_value(field_name)  # 返回使用输出处理器为给定字段解析的收集值。
l.get_value(value, *processors, kw)  # 处理给定的 value 被给予processors 关键字参数。
l.get_xpath(XPath, *processors, kw)  # 用于与此相关的选择器中提取 Unicode 字符串的列表。
l.load_item()  # 收集的数据填充并返回。
l.nested_css(CSS, context)  # 使用 CSS 选择器创建嵌套加载器。
l.nested_xpath (XPath, context)  # 使用XPath选择器创建嵌套加载器。
l.replace_css(field_name, CSS, *processors, kw)  # 类似于add_css()但取代收集的数据而不是添加数据。
l.replace_value(field_name, 价值, *processors, kw)  # 类似于 add_value() 但是将收集到的数据替换为新值，而不是添加它。
l.replace_xpath(field_name, XPath, *processors, kw)  # 类似于 add_xpath() 但取代收集的数据而不是添加数据。
```
### pipline
**setting配置**
```python
ITEM_PIPELINES = {
   'test.pipelines.Pipeline': 300,
}
```
**存储pipline**
```python
import pymongo
from itemadapter import ItemAdapter

class MongoPipeline:
    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db
    
    # 当创建一个pipline实例的时候该方法会被调用，必须返回一个pipline实例对象，一般在项目setting中配置。
    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )
    
    # 打开Spider时调用此方法
    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    # 关闭Spider时调用此方法
    def close_spider(self, spider):
        self.client.close()

    # 处理Items
    def process_item(self, item, spider):
        # 处理不同items
        if isinstance(item, TestItem):
            self.db['collection'].insert_one(ItemAdapter(item).asdict())
        else:
            pass
        return item
```
**去重pipline**
```python
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

class DuplicatesPipeline:

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter['id'] in self.ids_seen:
            raise DropItem(f"重复 item found: {item!r}")
        else:
            self.ids_seen.add(adapter['id'])
            return item
```
### middlewares
**setting配置**
```python
DOWNLOADER_MIDDLEWARES = {
   'test.middlewares.ProxyMiddleware': 500,
}
SPIDER_MIDDLEWARES = {
   'test.middlewares.testSpiderMiddleware': 543,
}
```
**代理中间件**
```python
class ProxyMiddleware:
    def __init__(self):
        self.ConnProxy = ConnProxy
    
    # requests中间件
    def process_request(self, request, spider):
        try:
            proxies = self.ConnProxy.get_proxies()
            request.meta['proxy'] = proxies
        except:
            print('get proxy fail!')
            spider.logger.error('get proxy fail!')
    
    # response中间件
    def process_response(self, request, response, spider):
        pass
    
    # 异常处理
    def process_exception(request, exception, spider):
        pass
    
```
## scrapyd
**安装**
```shell
pip install scrapyd
pip install scrapy-client
```
### 发布
**scrapy.cfg**
```ini
[settings]
default = test.settings

[deploy:dp]  # 服务器名
url = http://localhost:6800/
project = test
```
```shell
# 查看当前目录下的可以使用的部署方式
scrapyd-deploy -l

# 部署，在scrapy.cfg目录下执行
scrapyd-deploy dp -p test --version 0.1
```
### API
```python
import requests
# 检查服务的负载状态。
def daemonstatus():
    url = 'http://localhost:6801/daemonstatus.json'
    response = requests.get(url)
    print(response.text)

# 部署项目
def addversion():
    url = 'http://120.27.34.25:6800/addversion.json'

# 调度爬虫
def schedule(project, version, spider):
    data = {
        'project': project,
        '_version': version,
        'spider': spider
    }
    url = 'http://localhost:6801/schedule.json'
    response = requests.post(url, data=data)

# 查看任务列表
def listjobs(project):
    url = 'http://localhost:6801/listjobs.json?project={}'.format(project)
    response = requests.get(url)
    
# 取消任务
def cancel(project, job):
    data = {
        'project': project,
        'job': job,
    }
    url = 'http://localhost:6801/cancel.json'
    response = requests.post(url, data=data)

# 查看项目列表
def listprojects():
    url = 'http://localhost:6801/listprojects.json'
    response = requests.get(url)

# 查看爬虫列表
def listspiders(project):
    url = 'http://localhost:6801/listspiders.json?project={}'.format(project)
    response = requests.get(url)

# 查看版本列表
def listversions(project):
    url = 'http://localhost:6801/listversions.json?project={}'.format(project)
    response = requests.get(url)

# 刪除版本
def delversion(project, version):
    url = 'http://localhost:6801/delversion.json'
    data = {
        'project': project,
        'version': version,
    }
    response = requests.post(url, data)

# 刪除项目
def delproject(project):
    url = 'http://120.27.34.25:6800/delproject.json'
    data = {
        'project': project,
    }
    response = requests.post(url, data=data)
```
## scrapy-redis
维护爬取队列：将request对象序列化转成字符串存储。  
三种队列：fifoqueue：先进先出，lifoqueue：先进后出，priorityqueue：优先级队列。  
去重：利用redis集合作为指纹集合，生成新request后与集合中的指纹对比。